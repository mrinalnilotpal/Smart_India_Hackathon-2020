{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SIH_Deep_Neural_Network.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOWIKK2sWHeE0nZCLGEBLDQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hari-19/Smart_India_Hackathon-2020/blob/master/SIH_Deep_Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApTEzLDLLYrb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ssUL1pebAV_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Data Set needs to be Loaded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9W-lhE9FNJ_J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Returns tanh of Data\n",
        "\n",
        "def tanh(data):\n",
        "  data = np.tanh(data)\n",
        "  return data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vVmAGTKOXN2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Returns ReLU of Data\n",
        "\n",
        "def relu(data):\n",
        "    data = np.where(data>0,data,0)\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQHaipuzfpK4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Returns ReLU of Data\n",
        "\n",
        "def sigmoid(data):\n",
        "    data = 1 + np.exp(-data)\n",
        "    data = 1/data\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MUavT2SQ7zZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Parameter initialization\n",
        "\n",
        "    # Arguments:\n",
        "    # layer_dims -- python array (list) containing the dimensions of each layer in the network\n",
        "    \n",
        "    # Returns:\n",
        "    # parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
        "    #                 Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
        "    #                 bl -- bias vector of shape (layer_dims[l], 1)\n",
        "\n",
        "def initialize_parameters(layer_dims):\n",
        "    parameters={}\n",
        "    L = len(layer_dims)            # number of layers in the network\n",
        "\n",
        "    for l in range(1, L):\n",
        "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
        "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1)) \n",
        "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
        "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
        "\n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQjmONqKeBXD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    # The linear part of a layer's forward propagation.\n",
        "\n",
        "    # Arguments:\n",
        "    # A -- activations from previous layer : (size of previous layer, number of examples)\n",
        "    # W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
        "    # b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
        "\n",
        "    # Returns:\n",
        "    # Z -- the input of the activation function\n",
        "    # cache -- a python tuple containing \"A\", \"W\" and \"b\"\n",
        "\n",
        "def linear_forward(A, W, b):\n",
        "    Z = np.dot(W,A) + b\n",
        "\n",
        "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
        "    cache = (A, W, b)\n",
        "    \n",
        "    return Z, cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mssn2XNtefec",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    # The forward propagation for the LINEAR->ACTIVATION layer\n",
        "\n",
        "    # Arguments:\n",
        "    # A_prev -- activations from previous layer : (size of previous layer, number of examples)\n",
        "    # W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
        "    # b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
        "    # activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\" or \"tanh\"(will be coded later)\n",
        "\n",
        "    # Returns:\n",
        "    # A -- the output of the activation function\n",
        "    # cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n",
        "\n",
        "def linear_activation_forward(A_prev, W, b, activation):\n",
        "   \n",
        "    if activation == \"sigmoid\":\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        A = sigmoid(Z)\n",
        "    \n",
        "    elif activation == \"relu\":\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        A = relu(Z)\n",
        "    \n",
        "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
        "    cache = (linear_cache, Z)\n",
        "\n",
        "    return A, cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPz2XqBhf-ig",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}